{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science for prediction machine:\n",
    "\n",
    "The data in the database came somewhat scrambled\n",
    "\n",
    "Problems with the original data:\n",
    "1. Incomplete (some values were missing in some rows)\n",
    "2. There are ',' inside some of the string values (this is a comma separated list)\n",
    "3. The dataset came with both parquet and csv files\n",
    "\n",
    "The first approach was to use the csv data loaded to python with pandas. The problem was that each year had more than 1.5 to 2.5Gb of data *each* so no laptop with 16Gb of ram could load such a dataset **in memory**. This lead to a clear path, use chunking (with the help of [dask](https://www.dask.org/)). Unfortunately (or fortunately, as well see next) this did not work, the \"read\" dataframe did not contain all of the information. \n",
    "\n",
    "In trying to figure out if dask or the csv was the problem we tried to merge the data from 2 different years and get some data from it with linux tools (it seems that python does not like to parse and write more than 10 million records) such as [awk](https://en.wikipedia.org/wiki/AWK), [uniq](https://en.wikipedia.org/wiki/Uniq), etc... What we found was that some of the string data contained the same character that was used to as the separator in the file (','). \n",
    "\n",
    "So, after much work trying to wrangle the data in the csvs to no avail (no parser did the job correctly) we tried to see what these [.parquet](https://parquet.apache.org/) files were. As it seems they are a columnar file (from apache) that does not store all of the data as strings. What this does is reduce the size need to store the data manyfold. To give an example our dataset consisting of two years wasted 3.6Gb of space as csv but has parquet occupied just under 400Mb. Nothing is a silver bullet tho and, as a last gripe by the gods against me, while merging the two parquet files I found that the values of one single column were set as `int64` in one table and `float64` in another... Fortunately this had a decently straightforward solution (casting one of the tables according to the schema of the other).\n",
    "\n",
    "We can **finally** load the data into memory to see what it is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
